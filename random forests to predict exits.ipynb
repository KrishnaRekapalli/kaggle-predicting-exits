{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "I have applied randomForests in sklearn on the dataset to predict the exits. The accuracy of the trained model on test data is shocking. The reason may be it is simulated data or the variability in the data is so low that 15k data points are way ore than enough to capture the entire randomness in the behaviour of employees. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n# Load scikit's random forest classifier library\nfrom sklearn.ensemble import RandomForestClassifier",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# R\n#Read the input Data\nrawDataDf = pd.read_csv('../input/HR_comma_sep.csv')\n\nrawDataDf.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# See the column names to select the feature columns that we want\nlist(rawDataDf.columns.values)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#We split the data into training and test\nrawDataDf['isTrain'] = np.random.uniform(0,1,len(rawDataDf)) <= 0.75",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "rawDataDf.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# The columns sales and salary are categorical we need to facorize them\nrawDataDf['sales_fac'] = pd.factorize(rawDataDf['sales'])[0]\nrawDataDf['salary_fac'] = pd.factorize(rawDataDf['salary'])[0]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# The columns sales and salary are categorical we need to facorize them\ntrain, test = rawDataDf[rawDataDf['isTrain']==True] , rawDataDf[rawDataDf['isTrain']==False]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#\nprint('The training set has ',len(train),' rows and test set has' ,len(test), 'rows')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "featureCols = ['satisfaction_level',\n               'last_evaluation',\n               'number_project',\n               'average_montly_hours',\n               'time_spend_company',\n               'Work_accident',\n               'promotion_last_5years',\n               'sales_fac',\n               'salary_fac']\nrespCol = 'left'",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Create a random forest classifier. By convention, clf means 'classifier'\nI have gone through the documentation of sklearn \nSome parameters I want to play with are:\n    n_estimators: The number of trees  \n    criterion : The function to measure quality of split. Accepts 'gini' or 'entrophy'",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "effArray = []\n#for n in range(3,51):\ndef rfClassifier(n_estimators,criterion):\n    \n    clf = RandomForestClassifier(n_estimators= n_estimators, criterion=criterion)\n    clf.fit(train[featureCols], train[respCol])\n\n    predPerformanceTable = pd.DataFrame(columns=['predictions','groundTruths'])\n    predPerformanceTable['predictions'] = clf.predict(test[featureCols])\n    predPerformanceTable['groundTruths'] = list(test[respCol])\n\n    predPerformanceTable['success'] = predPerformanceTable['predictions'] == predPerformanceTable['groundTruths']\n\n    hits = len(predPerformanceTable[predPerformanceTable['success']==True])\n    misses = len(test) - hits\n    predictionAccuracy =  hits/len(test)\n    #effArray.append(predictionAccuracy)\n    \n    # Lets count the type I and type II errors in prediction\n    \n    # True Positives and True Negatives\n    tp = len(predPerformanceTable[(predPerformanceTable['predictions']==1)&(predPerformanceTable['groundTruths']==1)])\n    tn = len(predPerformanceTable[(predPerformanceTable['predictions']==0)&(predPerformanceTable['groundTruths']==0)])\n    \n    # Type I and Type II errors\n    # fp: Type I error\n    # fn: Type II error\n    \n    fp = len(predPerformanceTable[(predPerformanceTable['predictions']==1)&(predPerformanceTable['groundTruths']==0)])\n    fn = len(predPerformanceTable[(predPerformanceTable['predictions']==0)&(predPerformanceTable['groundTruths']==1)])\n    \n    \n    # calculating precision and recall\n    \n    # precision: tp/(tp+fp) \n    # What % of the positives that the model predicts are really positives\n    \n    precision = tp/(tp+fp)\n    \n    # recall: \n    # What % of relevant items are selected\n    recall = tp/(tp+fn)\n    \n    \n    result = { 'accuracy': predictionAccuracy,\n               'performanceTable':predPerformanceTable,\n               'precision':precision,\n               'recall':recall,\n               'typeI': fp,\n               'typeII': fn}\n    \n    return result",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "criterion = 'entropy'\neffArray = []\nprecisionArray = []\nrecallArray= []\n\nfor n in range(1,101):\n    \n    res = rfClassifier(n_estimators=n, criterion=criterion)\n    \n    effArray.append(res['accuracy'])\n    precisionArray.append(res['precision'])\n    recallArray.append(res['recall'])\n    \n    #print('Trees in Forest: ',n)\n    #print('False Positives: ', res['typeI'], ', False Negatives: ', res['typeII'])\n    \n    \n    \n    #xAxis = np.arange(3,51)\n\n    #plt.scatter(xAxis, effArray)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# I want to see the effect of number of trees in the forest on the prediction accuracy\n# we can see that after a point the accuracy doesnt improve much\n# Somewhere arounf 20 trees the three metrics stabilize and\n# it is not beneficial to go for more trees as the metrics reached pleateau\nxaxis = np.arange(1,101)\nplt.plot(xaxis, effArray,label='accuracy')\nplt.plot(xaxis, precisionArray,label='precision')\nplt.plot(xaxis, recallArray,label='recall')\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# original data composition\nexits = len(rawDataDf[rawDataDf['left']==1])\nstays = len(rawDataDf) - exits\nprint( exits, ' is the number of employees who left ')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\ntrain_imp = train.drop(\"left\", axis=1)\n\nimportances = random_forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in random_forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(train_imp.shape[1]):\n    print(\"%d. %s (%f)\" % (f + 1, train_imp.columns[indices[f]], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(10, 5))\nplt.title(\"Feature importances\")\nplt.bar(range(train_imp.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(train_imp.shape[1]), train_imp.columns[indices], rotation='vertical')\nplt.xlim([-1, train_imp.shape[1]])\nplt.show()",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## The following analysis is adopted from notebook by Ilana Radinsky\n## Thanks for the insightful work\n\n\nclf = RandomForestClassifier(n_estimators= 101, criterion='entropy')\nclf.fit(train[featureCols], train[respCol])\nimportances = clf.feature_importances_\n\nstd = np.std([tree.feature_importances_ for tree in clf.estimators_],\n             axis=0)\n\nindices = np.argsort(importances)[::-1]\n\n\nfor f in range(len(featureCols)):\n    print(\"%d. %s (%f)\" % (f + 1, featureCols[indices[f]], importances[indices[f]]))\n\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(10, 5))\nplt.title(\"Feature importances\")\nplt.bar(range(len(featureCols)), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(featureCols)), featureCols, rotation='vertical')\nplt.xlim([-1, len(featureCols)])\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Top Factors that determine the employee exit",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "1. satisfaction_level (0.323033)\n2. time_spend_company (0.196512)\n3. number_project (0.160477)\n4. average_montly_hours (0.155055)\n5. last_evaluation (0.125988)\n\nThese factors 90% of the time determine the employee's decision to stay or leave.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## What more can be done?\nSeeing how these top factors affect the decision. The relation between the exit and the direction of the factors",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}